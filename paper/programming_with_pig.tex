\section{Programming With Pig}

Pig offers an interactive shell called Grunt and a command line tool for running external scripts. Both ways work in local mode (Pig standalone) and on a Hadoop cluster.

\begin{lstlisting}[language=pig,caption=A typical Pig line of code,label=pigsample]
ordered = ORDER words;
\end{lstlisting} 

The code in listing \ref{pigsample} shows the typical structure of a pig statement. It is the data transformation of one set (``{\tt words}'') with an operation (``{\tt ORDER}'') into a new set (``{\tt ordered}''). The input data sets are always on the right side of the assignment. The resulting output data set is always on the left side.

Most often Pig Latin Scripts naturally follow the structure of listing \ref{pigstructure}.

\begin{lstlisting}[language=jaql,caption=Pig Latin Script Structure ,label=pigstructure]
Load Data -> Manipulate Data -> Group Data -> Output Data
\end{lstlisting}
                                                       
A Pig Latin script always begins with a {\tt LOAD} statement. The loaded data is manipulated via {\tt FILTER}, {\tt FOREACH GENERATE} or {\tt DISTINCT} statements or with the help of user-defined functions (UDF). Afterwards the data is often grouped. Then the cycle either starts all over again e.g. with additional data being joined or otherwise the resulting data set is written to the output with a {\tt STORE} statement. Of course this structure may be altered depending on the problem.     

Every line is terminated by a semicolon. The pig compiler starts building a logical and physical execution plan once a {\tt STORE} or debug command is evaluated.
                   
Every data set has a schema determining the structure of its data. These schemas may be defined explicitly by the user in Pig Latin or in an UDF or determined implicitly by Pig. A certain attribute in a data set is accessed either via the corresponding name in the schema or via its position.
As already stated Pig has the same possibilities as native Java Hadoop due to the use of UDFs. These functions may implement custom load or store functionality or manipulate data per element or over a whole data set. It is also possible to use UDFs for filtering by deciding if certain data shall be in the resulting data set.
The development of UDFs is well documented~\cite{pigUdf} but requires noteworthy more effort than just writing Pig Latin. Especially defining own schemas can be a hard-to-debug task. The development of UDFs offers common means of Java debugging.

Pig Latin scripts are best to be debugged with the {\tt ILLUSTRATE}, {\tt DESCRIBE} and {\tt DUMP} statements, which give access to the schema and the data. Unfortunately schemas are only to be viewed one level deep and therefore deeper nested data structures are to be debugged more difficult.