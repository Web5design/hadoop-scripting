\section{Introduction}

Working with big data sets has gained growing significance in the past years. This means processing heterogenous Gigabytes or terabytes of plain text by filtering, grouping or applying functions on the whole set or elements. One solution to this problem is using the Map/Reduce paradigm, which enables a cluster of computers to process a problem parallel and thus increase the processing speed.

Apache Hadoop is an Open Source Implementation of Map/Reduce. It is published under the Apache License and is maintained by the Apache Foundation. It's development is mostly driven by Yahoo! employers.

Hadoop programs are written in Java against the Hadoop API. On top of Hadoop, several domain specific languages try to establish a more abstract approach to the Map/Reduce paradigm. We have chosen Pig and Jaql to compare them in feature set, ease of programming and processing speed.

All source code written in running the benchmarks for this paper are published at GitHub under Apache License.