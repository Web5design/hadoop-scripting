Working with big data sets has gained growing significance in the past years. This means processing heterogenous Gigabytes or terrabytes of plain text by filtering, grouping or applying functions on the whole set or elemtens. One solution to this problem is using the Map/Reduce paradigm, which enables a cluster of computers to process a problem parallel and thus increase the processing speed.

Apache Hadoop is an Open Source Implementation of Map/Reduce. It is published under the Apache License and is maintained by the Apache Foundation. It's development is mostly driven by Yahoo! employers.

Hadoop programms are written in Java against the Hadoop API. On top of Hadoop, several domain specific languages try to establish a more abstract approach to the Map/Reduce paradigm. We have choosen Pig and Jaql to compare them in feature set, ease of programming and processing speed.

This paper has been written in the Map/Reduce lecture by Prof. Felix Naumann at Hasso Plattner Institute Potsdam Germany.